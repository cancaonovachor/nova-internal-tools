"""Vertex AI Agent Engineç”¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®šç¾©ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ï¼‰"""

import json
import os

import requests
from google import genai
from google.adk.agents import Agent
from google.genai import types

from scraper.api_tools import (
    fetch_article_content,
    fetch_jcanet_news,
    fetch_panamusica_news,
)


def _extract_and_explain_proper_nouns(title: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å›ºæœ‰åè©ã‚’æŠ½å‡ºã—è§£èª¬ã‚’ç”Ÿæˆ"""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        return ""

    client = genai.Client(api_key=api_key)

    extract_prompt = f"""ä»¥ä¸‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€åˆå”±éŸ³æ¥½ã«é–¢é€£ã™ã‚‹å›ºæœ‰åè©ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}

ã€æŠ½å‡ºå¯¾è±¡ã€‘
- äººåï¼ˆä½œæ›²å®¶ã€æŒ‡æ®è€…ã€æ­Œæ‰‹ãªã©ï¼‰
- åˆå”±å›£ãƒ»ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ©å
- ä½œå“åãƒ»æ›²å
- éŸ³æ¥½ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ•ã‚§ã‚¹ãƒ†ã‚£ãƒãƒ«å

ã€æŠ½å‡ºã—ãªã„ã‚‚ã®ã€‘
- æœˆåã€æ›œæ—¥ã€å¹´å·
- ä¸€èˆ¬çš„ãªå ´æ‰€å
- æ™®é€šåè©ã‚„å½¢å®¹è©

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{"proper_nouns": ["å›ºæœ‰åè©1", "å›ºæœ‰åè©2", ...]}}

å›ºæœ‰åè©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

    try:
        extract_response = client.models.generate_content(
            model="gemini-2.0-flash-lite-preview-02-05",
            contents=extract_prompt,
            config=types.GenerateContentConfig(response_mime_type="application/json"),
        )

        extract_text = extract_response.text.strip()
        if extract_text.startswith("```"):
            extract_text = extract_text.split("\n", 1)[1]
            if extract_text.endswith("```"):
                extract_text = extract_text.rsplit("\n", 1)[0]

        extract_result = json.loads(extract_text.strip())
        proper_nouns = extract_result.get("proper_nouns", [])

        if not proper_nouns:
            return ""

        search_prompt = f"""ä»¥ä¸‹ã®å›ºæœ‰åè©ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œ1-2æ–‡ã§ç°¡æ½”ã«æ—¥æœ¬èªã§è§£èª¬ã—ã¦ãã ã•ã„ã€‚
åˆå”±éŸ³æ¥½ã‚„éŸ³æ¥½ã«é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å„ªå…ˆã—ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

å›ºæœ‰åè©: {', '.join(proper_nouns)}

ã€é‡è¦ãªãƒ«ãƒ¼ãƒ«ã€‘
- å‰ç½®ãã‚„æŒ¨æ‹¶ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã“ã¨
- è§£èª¬ã¯å¿…ãšæ—¥æœ¬èªã§æ›¸ãã“ã¨
- ä»¥ä¸‹ã®å½¢å¼ã®ã¿ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ï¼š

ãƒ»å›ºæœ‰åè©å: è§£èª¬æ–‡

ã‚ã‹ã‚‰ãªã„å ´åˆã‚„ä¸€èˆ¬çš„ã™ãã‚‹å˜èªã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚"""

        search_response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=search_prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())],
            ),
        )

        return search_response.text.strip()

    except Exception as e:
        print(f"Proper noun extraction error: {e}")
        return ""


def send_discord_notification(
    title: str, summary: str, url: str, source: str, date: str
) -> dict:
    """è¦ç´„ã—ãŸè¨˜äº‹ã‚’Discordã«é€šçŸ¥ã™ã‚‹ã€‚"""
    discord_webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    if not discord_webhook_url:
        return {"status": "error", "message": "DISCORD_WEBHOOK_URL is not set"}

    # å›ºæœ‰åè©ã®è§£èª¬ã‚’å–å¾—
    explanations = _extract_and_explain_proper_nouns(title)
    explanation_section = ""
    if explanations:
        explanation_section = f"""

ğŸ“š **ç”¨èªè§£èª¬**
{explanations}"""

    message = f"""ğŸ“° **{source}** ã®æ–°ç€è¨˜äº‹
ğŸ“† **å…¬é–‹æ—¥**: {date}
ğŸ“„ **ã‚¿ã‚¤ãƒˆãƒ«**: {title}
ğŸ”— **URL**: {url}

ğŸ“ **è¦ç´„**
{summary}{explanation_section}
"""

    try:
        response = requests.post(discord_webhook_url, json={"content": message})
        response.raise_for_status()
        return {"status": "success", "message": f"Sent notification for: {title}"}
    except requests.exceptions.RequestException as e:
        return {"status": "error", "message": f"Failed to send: {str(e)}"}


root_agent = Agent(
    name="choral_news_scraper_agent",
    model="gemini-2.0-flash",
    description="åˆå”±é–¢é€£ã‚µã‚¤ãƒˆã®æ–°ç€æƒ…å ±ã‚’åé›†ã—ã€è¦ç´„ã—ã¦Discordã«é€šçŸ¥ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
    instruction="""ã‚ãªãŸã¯åˆå”±ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ãŸã‚ã®æƒ…å ±åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

ã€é‡è¦ã€‘å„è¨˜äº‹ã®è¦ç´„ã‚’ä½œæˆã™ã‚‹å‰ã«ã€å¿…ãšfetch_article_content()ã§è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚
ã‚¿ã‚¤ãƒˆãƒ«ã ã‘ã§è¦ç´„ã‚’æ¨æ¸¬ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

æ‰‹é †ï¼š
1. fetch_jcanet_news() ã§æ—¥æœ¬åˆå”±æŒ‡æ®è€…å”ä¼šã®æ–°ç€æƒ…å ±ã‚’å–å¾—
2. fetch_panamusica_news() ã§ãƒ‘ãƒŠãƒ ã‚¸ã‚«ã®ãŠçŸ¥ã‚‰ã›ã‚’å–å¾—
3. ã€å¿…é ˆã€‘å„è¨˜äº‹ã«ã¤ã„ã¦ fetch_article_content(url) ã§æœ¬æ–‡ã‚’å–å¾—
4. å–å¾—ã—ãŸæœ¬æ–‡ã‚’åŸºã«ã€3-4æ–‡ç¨‹åº¦ã§è¦ç´„ã‚’ä½œæˆ
5. send_discord_notification() ã§Discordã«é€šçŸ¥

å‡¦ç†ã™ã‚‹è¨˜äº‹æ•°ï¼šå„ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°3ä»¶ãšã¤
""",
    tools=[
        fetch_jcanet_news,
        fetch_panamusica_news,
        fetch_article_content,
        send_discord_notification,
    ],
)
