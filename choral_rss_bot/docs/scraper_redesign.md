# Web Scraper ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°è¨­è¨ˆ

## æ¦‚è¦

Agent Engine ã‚’å‰Šé™¤ã—ã€Cloud Run Jobs + LLM ã§ã‚·ãƒ³ãƒ—ãƒ«ã«å®Ÿè£…ã™ã‚‹ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### Before (ç¾çŠ¶)
```
Cloud Scheduler
       â†“
Agent Engine (Gemini) â† å‰Šé™¤å¯¾è±¡
       â†“
Cloud Run (Scraper API) â† å‰Šé™¤å¯¾è±¡
       â†“
Webã‚µã‚¤ãƒˆ â†’ Discord
```

### After (æ–°è¨­è¨ˆ)
```
Cloud Scheduler (æ—¥æ¬¡)
       â†“
Cloud Run Jobs
       â†“
Playwright + Gemini (HTMLè§£æãƒ»è¦ç´„)
       â†“
Webã‚µã‚¤ãƒˆ â†’ Discord
```

## å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
1. Cloud Run Jobèµ·å‹•
2. å¯¾è±¡ã‚µã‚¤ãƒˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—:
   a. Playwrightã§ãƒšãƒ¼ã‚¸å–å¾— â†’ HTMLå–å¾—
   b. Geminiã«HTMLæ¸¡ã—ã¦è¨˜äº‹ãƒªã‚¹ãƒˆæŠ½å‡º (JSON)
   c. å„è¨˜äº‹URLã«ã‚¢ã‚¯ã‚»ã‚¹ â†’ æœ¬æ–‡HTMLå–å¾—
   d. Geminiã§æœ¬æ–‡è¦ç´„
   e. é‡è¤‡ãƒã‚§ãƒƒã‚¯ (Firestore)
   f. Discordé€šçŸ¥ (å›ºæœ‰åè©è§£èª¬ä»˜ã)
   g. Firestoreã«å±¥æ­´ä¿å­˜
3. å®Œäº†
```

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

### å‰Šé™¤å¯¾è±¡
```
scraper/api.py           # FastAPI (ä¸è¦)
scraper/api_tools.py     # Agentç”¨ãƒ„ãƒ¼ãƒ« (ä¸è¦)
scraper/agent.py         # ãƒ­ãƒ¼ã‚«ãƒ«Agent (ä¸è¦)
agent_engine_agent.py    # Agent Engineå®šç¾© (ä¸è¦)
deploy/deploy_agent_engine.py  # Agent Engineãƒ‡ãƒ—ãƒ­ã‚¤ (ä¸è¦)
deploy/Dockerfile.web_scraper  # Cloud Run APIç”¨ (ä¸è¦)
deploy/cloudbuild.web_scraper.yaml  # (ä¸è¦)
```

### å¤‰æ›´å¯¾è±¡
```
scraper/main.py          # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ (å¤§å¹…ä¿®æ­£)
scraper/tools.py         # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (LLMçµ±åˆ)
```

### æ–°è¦ä½œæˆ
```
scraper/llm_helper.py    # LLMé–¢é€£å‡¦ç† (HTMLè§£æã€è¦ç´„)
scraper/config.yaml      # ã‚µã‚¤ãƒˆè¨­å®š
```

### æ—¢å­˜æµç”¨
```
common/storage.py        # Firestore (ãã®ã¾ã¾ä½¿ç”¨)
common/discord.py        # Discordé€šçŸ¥ (ãã®ã¾ã¾ä½¿ç”¨)
rss/llm_helper.py        # å›ºæœ‰åè©æŠ½å‡º (å‚è€ƒãƒ»æµç”¨)
```

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (scraper/config.yaml)

```yaml
sites:
  - id: jcanet
    name: æ—¥æœ¬åˆå”±é€£ç›Ÿ
    url: https://jcanet.or.jp/index.html
    max_articles: 5

  - id: panamusica
    name: ãƒ‘ãƒŠãƒ ã‚¸ã‚«
    url: https://panamusica.co.jp/ja/info/
    max_articles: 5

settings:
  max_history_items: 500
  article_age_days: 30
```

## LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

### 1. è¨˜äº‹ãƒªã‚¹ãƒˆæŠ½å‡º

```python
EXTRACT_ARTICLES_PROMPT = """
ä»¥ä¸‹ã®HTMLã‹ã‚‰æ–°ç€è¨˜äº‹ãƒ»ãŠçŸ¥ã‚‰ã›ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

HTML:
{html}

ã€æŠ½å‡ºå¯¾è±¡ã€‘
- æ–°ç€æƒ…å ±ã€ãŠçŸ¥ã‚‰ã›ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã©ã®ãƒªãƒ³ã‚¯
- ã‚¤ãƒ™ãƒ³ãƒˆå‘ŠçŸ¥ã€æ›´æ–°æƒ…å ±

ã€é™¤å¤–å¯¾è±¡ã€‘
- ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼
- ãƒ•ãƒƒã‚¿ãƒ¼ãƒªãƒ³ã‚¯
- SNSãƒªãƒ³ã‚¯
- åºƒå‘Š

ã€å‡ºåŠ›å½¢å¼ã€‘JSON:
{{
  "articles": [
    {{"title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«", "url": "https://...", "date": "2025/01/02"}},
    ...
  ]
}}

æ—¥ä»˜ãŒä¸æ˜ãªå ´åˆã¯ç©ºæ–‡å­—ã€‚æœ€å¤§{max_articles}ä»¶ã¾ã§ã€‚
"""
```

### 2. è¨˜äº‹æœ¬æ–‡æŠ½å‡º

```python
EXTRACT_CONTENT_PROMPT = """
ä»¥ä¸‹ã®HTMLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

HTML:
{html}

ã€æŠ½å‡ºå¯¾è±¡ã€‘
- è¨˜äº‹ã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
- é‡è¦ãªæƒ…å ±ï¼ˆæ—¥æ™‚ã€å ´æ‰€ã€è©³ç´°ï¼‰

ã€é™¤å¤–å¯¾è±¡ã€‘
- ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€åºƒå‘Š
- ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€ã‚¹ã‚¿ã‚¤ãƒ«

ã€å‡ºåŠ›å½¢å¼ã€‘ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆæœ€å¤§2000æ–‡å­—ï¼‰
"""
```

### 3. è¦ç´„ç”Ÿæˆ

```python
SUMMARIZE_PROMPT = """
ä»¥ä¸‹ã®è¨˜äº‹ã‚’3-4æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}
æœ¬æ–‡:
{content}

ã€ãƒ«ãƒ¼ãƒ«ã€‘
- æ—¥æœ¬èªã§è¦ç´„
- é‡è¦ãªæƒ…å ±ï¼ˆæ—¥æ™‚ã€å ´æ‰€ã€å†…å®¹ï¼‰ã‚’å«ã‚ã‚‹
- åˆå”±é–¢ä¿‚è€…ãŒèˆˆå‘³ã‚’æŒã¤ãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿
"""
```

## å®Ÿè£…è©³ç´°

### scraper/llm_helper.py

```python
"""Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨LLMãƒ˜ãƒ«ãƒ‘ãƒ¼"""

import json
import os
from google import genai
from google.genai import types

def extract_articles_from_html(html: str, max_articles: int = 5) -> list[dict]:
    """HTMLã‹ã‚‰è¨˜äº‹ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
    # Gemini APIå‘¼ã³å‡ºã—
    # JSONå½¢å¼ã§articlesã‚’è¿”ã™
    pass

def extract_content_from_html(html: str) -> str:
    """HTMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º"""
    pass

def summarize_article(title: str, content: str) -> str:
    """è¨˜äº‹ã‚’è¦ç´„"""
    pass

def extract_and_explain_proper_nouns(title: str) -> dict:
    """å›ºæœ‰åè©æŠ½å‡ºãƒ»è§£èª¬ï¼ˆrss/llm_helper.pyã‹ã‚‰æµç”¨ï¼‰"""
    pass
```

### scraper/tools.py

```python
"""Playwrightã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«"""

async def fetch_page_html(url: str) -> str:
    """ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—"""
    pass

async def scrape_site(site_config: dict) -> list[dict]:
    """ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    # 1. ãƒšãƒ¼ã‚¸HTMLå–å¾—
    # 2. LLMã§è¨˜äº‹ãƒªã‚¹ãƒˆæŠ½å‡º
    # 3. å„è¨˜äº‹ã®æœ¬æ–‡å–å¾—ãƒ»è¦ç´„
    pass
```

### scraper/main.py

```python
"""Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ¡ã‚¤ãƒ³"""

def main():
    # 1. è¨­å®šèª­ã¿è¾¼ã¿
    # 2. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆæœŸåŒ–
    # 3. ã‚µã‚¤ãƒˆã”ã¨ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    # 4. é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»Discordé€šçŸ¥
    pass
```

## Discordé€šçŸ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

```
ğŸ“° ã€{source}ã€ã®æ–°ç€è¨˜äº‹ã§ã™ï¼
ğŸ“†å…¬é–‹æ—¥æ™‚: {date}
ğŸ“„ã‚¿ã‚¤ãƒˆãƒ«: {title}
ğŸ”—ãƒªãƒ³ã‚¯: {url}

ğŸ“ è¦ç´„
{summary}

ğŸ“š ç”¨èªè§£èª¬
{explanations}
```

## ãƒ‡ãƒ—ãƒ­ã‚¤

### Dockerfile

```dockerfile
FROM mcr.microsoft.com/playwright/python:v1.57.0-noble
# æ—¢å­˜ã®Dockerfile.web_scraperã‚’æµç”¨ã€CMDå¤‰æ›´
CMD ["uv", "run", "python", "-m", "scraper.main", "--mode", "discord"]
```

### Cloud Run Jobs

```bash
gcloud run jobs create choral-web-scraper \
  --image=REGION-docker.pkg.dev/PROJECT/REPO/choral-web-scraper:latest \
  --region=asia-northeast1 \
  --set-env-vars="DISCORD_WEBHOOK_URL=...,GEMINI_API_KEY=..."
```

### Cloud Scheduler

```bash
gcloud scheduler jobs create http choral-web-scraper-daily \
  --schedule="0 9 * * *" \
  --uri="https://REGION-run.googleapis.com/apis/run.googleapis.com/v1/..." \
  --http-method=POST
```

## å®Ÿè£…é †åº

1. `scraper/llm_helper.py` ä½œæˆ (HTMLè§£æãƒ»è¦ç´„)
2. `scraper/tools.py` ä¿®æ­£ (LLMçµ±åˆ)
3. `scraper/config.yaml` ä½œæˆ
4. `scraper/main.py` ä¿®æ­£ (rss/main.pyå‚è€ƒ)
5. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ
6. ä¸è¦ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
7. Dockerfileæ›´æ–°
8. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»å‹•ä½œç¢ºèª
9. Agent Engineå‰Šé™¤
